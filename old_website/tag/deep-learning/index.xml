<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Deep Learning | Anurag Dutt</title>
    <link>https://anuragdutt.github.io/tag/deep-learning/</link>
      <atom:link href="https://anuragdutt.github.io/tag/deep-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Deep Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 27 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://anuragdutt.github.io/media/avatar.jpg</url>
      <title>Deep Learning</title>
      <link>https://anuragdutt.github.io/tag/deep-learning/</link>
    </image>
    
    <item>
      <title>Comparison of Adverserial techniques for synthesizing electronic health records - Data Management and Biomedical DataAnalytics Lab, SBU</title>
      <link>https://anuragdutt.github.io/project/gan_comp/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/gan_comp/</guid>
      <description>&lt;p&gt;This project was undertaken as a part of our paper &amp;ldquo;SMOOTH-GAN: Towards Sharpand Smooth Synthetic EHR DataGeneration&amp;rdquo; to compare the performance of SmoothGAN architecture for generation of Electronic Health Records. We compare the performance of our architecture with other discrete distribution architectures such as MedGAN, WGAN, WGAN-GP, AC-GAN and cGAN.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Multi-Agent Generative Adversarial Self-Imitation Learning (Graduate project)</title>
      <link>https://anuragdutt.github.io/project/gasil/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/gasil/</guid>
      <description>&lt;p&gt;In GANs, a discriminator is trained to discriminate whether a given sample is drawn from data distribution or model distribution. A generator (i.e., model) is trained to “fool” the discriminator by generating samples that are close to the real data. In reward learning true reward function may not be optimal or even known, GASIL learns a discriminator which acts as an internal reward function that the policy should maximize. In self imitation learning the agent directly from the past experience by retrieving similar states in the past and choosing the best action made in the past. In GASIL architecture, the agent directly learns to imitate past good trajectories without learning a generative model. GASIL can be viewed as a generative adversarial extension of self-imitation learning. The GASIL approach stores and imitates
the high reward trajectories seen in the past, and imitates them in the Adversarial framework.
GASIL could be complemented with the Q-Learning or Policy gradient in the control part of the
reinforcement learning. We implemented the GASIL in multi-agent environment and our results show
that the GASIL outperforms the basline DDPG model.s&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Using Natural Language Processing and Deep Learning Techniques on 8-k reports for stock price movement prediction - Language Understanding and Reasoning Lab, SBU</title>
      <link>https://anuragdutt.github.io/project/spred_nlp/</link>
      <pubDate>Fri, 27 Dec 2019 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/spred_nlp/</guid>
      <description>&lt;p&gt;In this work we test if financial reports can be utilized to research the significance of text analysis for stock price prediction.  Our corpus adjustsportrayals of money related occasions announcedin 8-K records with the relating stock costs, whichencourages  the  improvement  of  stock  price  esti-mation frameworks which joins textual and finan-cial  data.   Utilizing  the  corpus,  we  demonstratethat  using  textual  data  has  a  significant  impactespecially  for  short-term  periods.(the  two  dayspromptly  following  the  occasion). We see that on applying our Bi-LSTM based dependency parser over then RNN-CNN framework, there is improovement to some extent in the performanceof the model as compared to only the CNN or the RNN based model. We believe that this is so because CNN’s pooling activity on nearby words can hold the neighborhood highlights and their consecutive relations  in  a  sentence and,  the RNN  can  learn the  long-term  dependencies  and  the  positional relation of features as well as the global features of the whole sentence.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dropout - A Simple Way to Prevent GANs from Overfitting (Graduate Project - Columbia)</title>
      <link>https://anuragdutt.github.io/project/dropoutgan/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/dropoutgan/</guid>
      <description>&lt;p&gt;Overfitting is a serious problem when using deep neural networks with a large
number of parameters. Large networks also take more time to train and test, making it less
practical in the real world. To address this problem, Dropout is widely employed. By randomly
removing units and its connections from the network, the technique significantly reduces co-
adapting effects during training. In our report, we mainly investigate this technique using three
datasets, MNIST, CIFAR10, and CIFAR100, and compare the result with related papers. The
result suggests that Dropout leads to an increase of accuracy making predictions. Batch
Normalization is another powerful technique in deep learning. However, combining Dropout and
Batch Normalization together often result in a worse performance. We explore different
strategies dealing with this problem. At the end, we further apply these strategies to multi-adversarial GANs.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
