<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Generative Adversarial Imitation Learning | Anurag Dutt</title>
    <link>https://anuragdutt.github.io/tag/generative-adversarial-imitation-learning/</link>
      <atom:link href="https://anuragdutt.github.io/tag/generative-adversarial-imitation-learning/index.xml" rel="self" type="application/rss+xml" />
    <description>Generative Adversarial Imitation Learning</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Mon, 18 May 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://anuragdutt.github.io/media/avatar.jpg</url>
      <title>Generative Adversarial Imitation Learning</title>
      <link>https://anuragdutt.github.io/tag/generative-adversarial-imitation-learning/</link>
    </image>
    
    <item>
      <title>Multi-Agent Generative Adversarial Self-Imitation Learning (Graduate project)</title>
      <link>https://anuragdutt.github.io/project/gasil/</link>
      <pubDate>Mon, 18 May 2020 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/gasil/</guid>
      <description>&lt;p&gt;In GANs, a discriminator is trained to discriminate whether a given sample is drawn from data distribution or model distribution. A generator (i.e., model) is trained to “fool” the discriminator by generating samples that are close to the real data. In reward learning true reward function may not be optimal or even known, GASIL learns a discriminator which acts as an internal reward function that the policy should maximize. In self imitation learning the agent directly from the past experience by retrieving similar states in the past and choosing the best action made in the past. In GASIL architecture, the agent directly learns to imitate past good trajectories without learning a generative model. GASIL can be viewed as a generative adversarial extension of self-imitation learning. The GASIL approach stores and imitates
the high reward trajectories seen in the past, and imitates them in the Adversarial framework.
GASIL could be complemented with the Q-Learning or Policy gradient in the control part of the
reinforcement learning. We implemented the GASIL in multi-agent environment and our results show
that the GASIL outperforms the basline DDPG model.s&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
