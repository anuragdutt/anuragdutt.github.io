<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Generative Adversarial Networks | Anurag Dutt</title>
    <link>https://anuragdutt.github.io/tag/generative-adversarial-networks/</link>
      <atom:link href="https://anuragdutt.github.io/tag/generative-adversarial-networks/index.xml" rel="self" type="application/rss+xml" />
    <description>Generative Adversarial Networks</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sun, 27 Sep 2020 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://anuragdutt.github.io/media/avatar.jpg</url>
      <title>Generative Adversarial Networks</title>
      <link>https://anuragdutt.github.io/tag/generative-adversarial-networks/</link>
    </image>
    
    <item>
      <title>Comparison of Adverserial techniques for synthesizing electronic health records - Data Management and Biomedical DataAnalytics Lab, SBU</title>
      <link>https://anuragdutt.github.io/project/gan_comp/</link>
      <pubDate>Sun, 27 Sep 2020 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/gan_comp/</guid>
      <description>&lt;p&gt;This project was undertaken as a part of our paper &amp;ldquo;SMOOTH-GAN: Towards Sharpand Smooth Synthetic EHR DataGeneration&amp;rdquo; to compare the performance of SmoothGAN architecture for generation of Electronic Health Records. We compare the performance of our architecture with other discrete distribution architectures such as MedGAN, WGAN, WGAN-GP, AC-GAN and cGAN.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Dropout - A Simple Way to Prevent GANs from Overfitting (Graduate Project - Columbia)</title>
      <link>https://anuragdutt.github.io/project/dropoutgan/</link>
      <pubDate>Fri, 21 Dec 2018 00:00:00 +0000</pubDate>
      <guid>https://anuragdutt.github.io/project/dropoutgan/</guid>
      <description>&lt;p&gt;Overfitting is a serious problem when using deep neural networks with a large
number of parameters. Large networks also take more time to train and test, making it less
practical in the real world. To address this problem, Dropout is widely employed. By randomly
removing units and its connections from the network, the technique significantly reduces co-
adapting effects during training. In our report, we mainly investigate this technique using three
datasets, MNIST, CIFAR10, and CIFAR100, and compare the result with related papers. The
result suggests that Dropout leads to an increase of accuracy making predictions. Batch
Normalization is another powerful technique in deep learning. However, combining Dropout and
Batch Normalization together often result in a worse performance. We explore different
strategies dealing with this problem. At the end, we further apply these strategies to multi-adversarial GANs.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
